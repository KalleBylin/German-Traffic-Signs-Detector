{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "source1 = \"images/train/\"\n",
    "dest11 = \"images/test/\"\n",
    "folders = os.listdir(source1)\n",
    "import shutil\n",
    "for folder in folders:\n",
    "    if not os.path.exists(dest11 + folder):\n",
    "        os.makedirs(dest11 + folder)\n",
    "    files = os.listdir(source1+folder)\n",
    "    for file in files:\n",
    "        if np.random.rand(1) < 0.2:\n",
    "            shutil.move(source1 + folder + '/' + file, dest11 + folder + '/'+ file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def load_img(path, size):\n",
    "    img = Image.open(path)\n",
    "    old_size = img.size\n",
    "\n",
    "    ratio = float(size/max(old_size))\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "\n",
    "    img = img.resize(new_size, Image.ANTIALIAS)\n",
    "    delta_w = size - new_size[0]\n",
    "    delta_h = size - new_size[1]\n",
    "    padding = (delta_w//2, delta_h//2, delta_w-(delta_w//2), delta_h-(delta_h//2))\n",
    "    img = ImageOps.expand(img, padding)\n",
    "    return np.array(img)\n",
    "    \n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "size = 28\n",
    "\n",
    "for j in range(43):\n",
    "    train_path = os.path.join('images', 'train', str(j).rjust(2, '0'), '*.ppm')\n",
    "    files = glob.glob(train_path)\n",
    "    for file in files:\n",
    "        X_train.append(load_img(file, size))\n",
    "        y_train.append(j)\n",
    "        \n",
    "    test_path = os.path.join('images', 'test', str(j).rjust(2, '0'), '*.ppm')\n",
    "    files = glob.glob(test_path)\n",
    "    for file in files:\n",
    "        X_test.append(load_img(file, size))\n",
    "        y_test.append(j)\n",
    "\n",
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "X_test = np.array(X_test, dtype=np.float32)\n",
    "y_test = np.array(y_test, dtype=np.float32)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19d5b1a6c18>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = Image.open('images/train/00/00001.ppm')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19d5b0f9630>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = Image.open('images/test/00/00000.ppm')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-Based Learning with LeNet model\n",
    "\n",
    "**What is it?**\n",
    "\n",
    "Pattern recognition systems perform better when built relying more on automatic learning and less on hand-designed heuristics. Usually the system is divided into two partes. First a feature extractor, which transforms the input patterns so that yjey can be represented by low-dimensional vectors that can be easily compared and are relatively invariant to transformations and distortions of the input patterns that don't change their nature. Secondly, a classifier which is often general-purpose and trainable.\n",
    "\n",
    "To train this system, gradient-based learning is often used. A loss function  which compares the desired output with the predictions of the model is minimized with procedures like gradient descent. This is possible in non-linear systems with several layers of processing thanks to algorithms like back-propagation used to calculate gradients efficiently by propagation from the output to the input. \n",
    "\n",
    "\n",
    "A real-world example for logistic regression could be as a predictor for wether a patient has a disease, like diabetes, or not, based on certain characteristics of the patient like age, body mass index, sex, results tests, etc.\n",
    "\n",
    "**Strengths of the model**\n",
    "\n",
    "Multi-layer networks trained with gradient descent are capable to learn high-dimensional and non-linear mappings from large collections of examples. This makes them suitable for image recognition tasks.\n",
    "\n",
    "The LeNet model uses a specific architecture known as Convolutional Networks. This type of netowrks combines local receptive fields, shared weights and spatial/temporal sub-sampling to ensure a certain degree of invariance to shift, scale and distortion in the input. The local receptive fields allow the network to extract elementary visual features that are combined in following layers to detect higher-order features.\n",
    "\n",
    "Convolutional networks are particularly well suited for recognizing or rejecting shapes with widely varying size, position, and orientation.\n",
    "\n",
    "**Weaknesses of the model**\n",
    "\n",
    "Still, as every model, it has its trade-offs and weaknesses. Logistic regression is a classification model for discrete, binary problems. In order to predecit multiple classes a scheme like one-vs-rest (OvR) has to be used. High correlation variables and outliers can make the model perform poorly. It sometimes needs large datasets because maximum likelihood estimates are less powerful than the ordinary least squares used for linear regression.\n",
    "\n",
    "**Fit for this problem**\n",
    "\n",
    "Logistic regression works well when there is a single decision boundary, which can be beneficial in a problem with binary labels like this one. With a little over 46.694 records in the training example it seems like the dataset is large enough for the model. Some statisticians recommend at least 10-30 cases for each parameter to be estimated (source: http://www.statisticssolutions.com/assumptions-of-logistic-regression).\n",
    "\n",
    "(Sources: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html, http://www.statisticssolutions.com/what-is-logistic-regression/, http://www.statisticssolutions.com/what-is-logistic-regression, http://www.statisticssolutions.com/assumptions-of-logistic-regression/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(972, 28, 28, 3)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(972, 2352)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2 = X_train.flatten().reshape(X_train.shape[0], size*size*3)\n",
    "X_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241, 2352)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test2 = X_test.flatten().reshape(X_test.shape[0], size*size*3)\n",
    "X_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "y_train2 = enc.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test2 = enc.transform(y_test.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(972, 43)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 0.0003\n",
    "num_input = X_train2.shape[1]\n",
    "num_classes = y_train2.shape[1]\n",
    "logs_path='/tmp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LeNet-5 architecture is described as follows:\n",
    "- Layer C1: Convolutional layer with 6 feature maps. Each unit in each feature map is connected to a 5x5 neighborhood in the input. \n",
    "- Layer S2: Sub-sampling layer with 6 feature maps of size 14x14. Each unit in the feature map is connected to a 2x2 neighborhood in the corresponding feature map in C1. The four inputs to a unit in S2 are added, then multiplied by a trainable coefficient, and added to a trainable bias. The result is passed through a sigmoidal function. \n",
    "- Layer C3: Convolutional layer with 16 feature maps. Each unit in each feature map is connected to several 5x5 neighborhoods at identical locations in a subset of S2's feature maps.\n",
    "- Layer S4: Sub-sampling layer with 16 feature maps of size 5x5. Each unit in each feature map is connected to a 2x2 neighborhood in the corresponding feature map in C3.\n",
    "- Layer C5: Convolutional layer with 120 feature maps. Each unit connected to a 5x5 neighborhood on all 16 of S4's feature maps. The size of C5's feature maps is 1x1. This amounts to a full connection between S4 and C5.\n",
    "- Layer F6: Fully connected layer with 84 units.\n",
    "- Output: Output units according to number of classes\n",
    "\n",
    "Units in layers up to F6 compute a dot product between their input vector and their weight vector, to which a bias is added. This weighted sum is then passed through a sigmoid squashing function. \n",
    "\n",
    "`tf.nn.conv2d` performs a 2D convolution with input and filter tensors:\n",
    "- An input tensor of shape [batch, in_height, in_width, in_channels]\n",
    "- A filter/kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "\n",
    "The sub-sampling layers described in the [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) paper compute the average of its inputs. \n",
    "\n",
    "`tf.nn.pool` performs a N-D pooling operation and takes a pooling_type parameter which can be specified as 'AVG' or 'MAX'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(input_data, input_channels, filter_size, num_filters):\n",
    "    W = tf.Variable(tf.truncated_normal(shape=[filter_size, filter_size, input_channels, num_filters], stddev=0.05))\n",
    "    b = tf.Variable(tf.constant(0.05, shape=[num_filters]))\n",
    "    \n",
    "    layer = tf.nn.conv2d(input=input_data, filter=W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    layer += b\n",
    "    \n",
    "    return layer\n",
    "\n",
    "def fully_connected(input_data, num_inputs, num_outputs):\n",
    "    W = tf.Variable(tf.truncated_normal([num_inputs, num_outputs], stddev=0.05))\n",
    "    b = tf.Variable(tf.constant(0.05, shape=[num_outputs]))\n",
    "    \n",
    "    return tf.matmul(input_data, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset tensorflow graph\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: (?, 28, 28, 3)\n",
      "C1: (?, 28, 28, 6)\n",
      "S2: (?, 14, 14, 6)\n",
      "A1: (?, 14, 14, 6)\n",
      "C3: (?, 14, 14, 16)\n",
      "S4: (?, 7, 7, 16)\n",
      "(?, 7, 7, 16)\n",
      "C5: (?, 7, 7, 120)\n",
      "F6: (?, 84)\n",
      "Output: (?, 43)\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 28, 28, 3], name='x_input')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_input')\n",
    "\n",
    "print('Input:', x.get_shape())\n",
    "c1 = conv_layer(input_data=x, input_channels=3, filter_size=5, num_filters=6)\n",
    "# We choose a window_size of [1, 2, 2, 1] for a 2x2 window but 1 for batch and \n",
    "# channel dimensions because we don't want to take the maximum over multiple examples or channels\n",
    "print('C1:', c1.get_shape())\n",
    "s2 = tf.nn.pool(input=c1, window_shape=[2, 2], pooling_type='AVG', padding='SAME', strides=[2,2])\n",
    "print('S2:', s2.get_shape())\n",
    "a1 = tf.nn.relu(s2)\n",
    "print('A1:', a1.get_shape())\n",
    "\n",
    "c3 = conv_layer(input_data=a1, input_channels=6, filter_size=5, num_filters=16)\n",
    "print('C3:', c3.get_shape())\n",
    "s4 = tf.nn.pool(input=c3, window_shape=[2, 2], pooling_type='AVG', padding='SAME', strides=[2,2])\n",
    "print('S4:', s4.get_shape())\n",
    "a2 = tf.nn.relu(s4)\n",
    "print(a2.get_shape())\n",
    "\n",
    "c5 = conv_layer(input_data=a2, input_channels=16, filter_size=5, num_filters=120)\n",
    "print('C5:', c5.get_shape())\n",
    "\n",
    "num_features = c5.get_shape()[1:4].num_elements()\n",
    "flat = tf.reshape(c5, [-1, num_features])\n",
    "f6 = fully_connected(input_data=flat, num_inputs=num_features, num_outputs=84)\n",
    "print('F6:', f6.get_shape())\n",
    "a3 = tf.nn.relu(f6)\n",
    "\n",
    "output = fully_connected(input_data=a3, num_inputs=84, num_outputs=num_classes)\n",
    "print('Output:', output.get_shape())\n",
    "\n",
    "with tf.name_scope('Output'):\n",
    "    y = tf.nn.softmax(output)\n",
    "\n",
    "with tf.name_scope(\"calculating_cost\"):\n",
    "    # calculating cost\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "with tf.name_scope(\"declaring_gradient_descent\"):\n",
    "    # optimizer\n",
    "    # we use gradient descent for our optimizer \n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Accuracy'):\n",
    "    prediction = tf.argmax(y, 1, name='predict')\n",
    "    correct = tf.equal(prediction, tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "randint() takes at least 1 positional argument (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-6fee42979b87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.randint\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: randint() takes at least 1 positional argument (0 given)"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Training accuracy: 0.04526749\n",
      "Test accuracy: 0.041493777\n",
      "Epoch 5\n",
      "Training accuracy: 0.047325104\n",
      "Test accuracy: 0.045643155\n",
      "Epoch 10\n",
      "Training accuracy: 0.047325104\n",
      "Test accuracy: 0.045643155\n",
      "Epoch 15\n",
      "Training accuracy: 0.047325104\n",
      "Test accuracy: 0.045643155\n",
      "Epoch 20\n",
      "Training accuracy: 0.047325104\n",
      "Test accuracy: 0.045643155\n",
      "Epoch 25\n",
      "Training accuracy: 0.047325104\n",
      "Test accuracy: 0.045643155\n",
      "Epoch 30\n",
      "Training accuracy: 0.047325104\n",
      "Test accuracy: 0.045643155\n",
      "Epoch 35\n",
      "Training accuracy: 0.047325104\n",
      "Test accuracy: 0.045643155\n",
      "Epoch 40\n",
      "Training accuracy: 0.047325104\n",
      "Test accuracy: 0.045643155\n",
      "Epoch 45\n",
      "Training accuracy: 0.047325104\n",
      "Test accuracy: 0.045643155\n",
      "Epoch 50\n",
      "Training accuracy: 0.047325104\n",
      "Test accuracy: 0.045643155\n",
      "Epoch 55\n",
      "Training accuracy: 0.047325104\n",
      "Test accuracy: 0.045643155\n",
      "Epoch 60\n",
      "Training accuracy: 0.047325104\n",
      "Test accuracy: 0.045643155\n",
      "Epoch 65\n",
      "Training accuracy: 0.047325104\n",
      "Test accuracy: 0.045643155\n",
      "Epoch 70\n",
      "Training accuracy: 0.047325104\n",
      "Test accuracy: 0.045643155\n",
      "Epoch 75\n",
      "Training accuracy: 0.047325104\n",
      "Test accuracy: 0.045643155\n",
      "Epoch 80\n",
      "Training accuracy: 0.047325104\n",
      "Test accuracy: 0.045643155\n",
      "Epoch 85\n",
      "Training accuracy: 0.047325104\n",
      "Test accuracy: 0.045643155\n",
      "Epoch 90\n",
      "Training accuracy: 0.047325104\n",
      "Test accuracy: 0.045643155\n",
      "Epoch 95\n",
      "Training accuracy: 0.047325104\n",
      "Test accuracy: 0.045643155\n",
      "Training finished\n",
      "Accuracy: 0.045643155\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        sess.run([optimizer, loss], feed_dict = {x: X_train, y_: y_train2})\n",
    "        \n",
    "        acc = sess.run(accuracy, feed_dict = {x: X_train, y_: y_train2})\n",
    "        test_acc = sess.run(accuracy, feed_dict = {x: X_test, y_: y_test2})\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch', epoch)\n",
    "            print('Training accuracy:', acc)\n",
    "            print('Test accuracy:', test_acc)\n",
    "    print('Training finished')\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print('Accuracy:', accuracy.eval({x: X_test, y_: y_test2}))\n",
    "    \n",
    "    #saver.save(sess, './ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = load_img('images/test/01/00014.ppm', 48)\n",
    "img = img.flatten().reshape(1, 6912)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ckpt\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.import_meta_graph('ckpt.meta')\n",
    "saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "\n",
    "print(sess.run(prediction, feed_dict = {x: img}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
